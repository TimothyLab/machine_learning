{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4127c170",
   "metadata": {},
   "source": [
    "# Regression avec un réseau de neurone simple pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0680c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotnine as pn\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from torchinfo import summary\n",
    "\n",
    "from adl.sklearn import skl_regression\n",
    "\n",
    "pn.theme_set(pn.theme_minimal())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a5268",
   "metadata": {},
   "source": [
    "Dans les tp précédents, on a utiliser la déscente de gradient pour résoudre de simple problème de régression linéaire. Ici nous allons introduire un réseau de neurone simple pour résoudre le même problème. On va également repartir des mêmes données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4eaad84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = [-1.5, 0.2, 3.4, 4.1, 7.8, 13.4, 18.0, 21.5, 32.0, 33.5]\n",
    "icecream = [100.5, 110.2, 133.5, 141.2, 172.8, 225.1, 251.0, 278.9, 366.7, 369.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e0abd8",
   "metadata": {},
   "source": [
    "Comme nous l'avons vu dans le tp sur la normalisation des données, on va normaliser la temperature pour améliorer l'entraînement. Et on va calculer les vrai solution , pente et ordonnée à l'origine avec scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee16cbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope: 95.03, intercept: 214.98, mse: 14.5403\n"
     ]
    }
   ],
   "source": [
    "temperature_s = preprocessing.scale(temperature, with_mean=True)\n",
    "\n",
    "reg = skl_regression(temperature_s, icecream)\n",
    "print(f\"slope: {reg['slope']:.2f}, intercept: {reg['intercept']:.2f}, mse: {reg['mse']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b07d8",
   "metadata": {},
   "source": [
    "Enfin, nous transformons nos données et nos valeur cibles en tensors. La différence ici, est que nous avons besoins d'organiser nos données de manière à avoir chaque oberservation et chaque valeur cible dans son propre tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7ce6513c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2238],\n",
      "        [-1.0827],\n",
      "        [-0.8170],\n",
      "        [-0.7589],\n",
      "        [-0.4517],\n",
      "        [ 0.0133],\n",
      "        [ 0.3952],\n",
      "        [ 0.6858],\n",
      "        [ 1.5576],\n",
      "        [ 1.6822]])\n",
      "tensor([[100.5000],\n",
      "        [110.2000],\n",
      "        [133.5000],\n",
      "        [141.2000],\n",
      "        [172.8000],\n",
      "        [225.1000],\n",
      "        [251.0000],\n",
      "        [278.9000],\n",
      "        [366.7000],\n",
      "        [369.9000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(temperature_s).float().view(-1, 1)\n",
    "y = torch.tensor(icecream).float().view(-1, 1)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ce148",
   "metadata": {},
   "source": [
    "# Régression avec pytorch et un réseau neuronal à neurone unique\n",
    "\n",
    "Dans les notebooks précédents, nous avons créé notre modèle en créant simplement une\n",
    "fonction `forward` simple, comme ceci :\n",
    "\n",
    "``` python\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "```\n",
    "\n",
    "Cette approche convient pour un modèle simple comme celui que nous avons actuellement, mais pour des\n",
    "modèles plus complexes tels que les réseaux neuronaux, nous devrons utiliser les\n",
    "fonctions intégrées de pytorch pour les définir.\n",
    "\n",
    "En fait, une régression linéaire simple avec une seule variable explicative\n",
    "peut être considérée comme un « réseau » neuronal avec un seul neurone. Nous allons donc\n",
    "essayer de réécrire notre modèle simple en utilisant la notation de pytorch.\n",
    "\n",
    "Une façon de définir notre « réseau » consiste à utiliser la notation *Module*,\n",
    "fournie par `torch.nn.Module`. Cette notation oblige à créer une nouvelle\n",
    "classe Python, qui hérite de `nn.Module`, puis à créer au\n",
    "moins une méthode `__init__()` (appelée lors de la création du modèle) et une\n",
    "méthode `forward()`, qui prend les données d'entrée comme argument, applique notre\n",
    "modèle et renvoie les valeurs prédites.\n",
    "\n",
    "Pour créer notre modèle de régression linéaire simple, nous utiliserons `nn.Linear`,\n",
    "qui permet de définir des couches linéaires de taille arbitraire. Ici, notre couche\n",
    "aura un seul neurone qui prendra un seul nombre en entrée (une\n",
    "valeur de température) et produira un seul résultat (un volume de vente de crème glacée prédit).\n",
    " En notation pytorch, cela signifie que notre couche aura\n",
    "`in_features` de taille 1 et `out_features` de taille 1.\n",
    "\n",
    "Voici le code d'une classe `LinearNetwork` qui implémente ce modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecace113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LinearNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Modèle de régression linéaire simple avec une seule variable d'entrée.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Call the parent constructor (mandatory)\n",
    "        super().__init__()\n",
    "        # Create a \"linear\" attribute which will contain a linear layer with input and\n",
    "        # output of size 1\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Méthode qui implémente la passe avant du modèle, c'est-à-dire qui prend les données\n",
    "        d'entrée en argument, applique le modèle et renvoie le résultat.\n",
    "        \"\"\"\n",
    "        # Apply our linear layer to input data\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f23df",
   "metadata": {},
   "source": [
    "Une fois que notre classe modèle est créer, on peut l'utiliser pour créer une instance de modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bef22e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "LinearNetwork                            --\n",
       "├─Linear: 1-1                            2\n",
       "=================================================================\n",
       "Total params: 2\n",
       "Trainable params: 2\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearNetwork()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f279e",
   "metadata": {},
   "source": [
    "Il est important de distinguer entre :\n",
    "\n",
    "-   une classe de modèle, telle que `LinearNetwork`, qui est une classe Python\n",
    "    décrivant une architecture de modèle\n",
    "-   un objet modèle ou une instance de modèle, tel que `model`, qui est un modèle concret\n",
    "    créé à l'aide de l'architecture `LinearNetwork`\n",
    "\n",
    "Nous pouvons utiliser la fonction `summary` du package `torchinfo` pour afficher\n",
    "une description de notre objet `model`.\n",
    "\n",
    "Nous pouvons voir que « model » comporte une couche et deux paramètres : le poids et\n",
    "le biais de notre « neurone » unique. Nous pouvons voir que PyTorch se charge de\n",
    "créer ces paramètres, nous n'avons plus besoin de créer manuellement les tenseurs « w » et « b »\n",
    ".\n",
    "\n",
    "Nous pouvons transmettre les données d'entrée directement à notre objet « model ». Dans ce cas, il appellera « model.forward() », qui applique le modèle aux données d'entrée\n",
    "pour calculer la prédiction. Nous pouvons voir que les deux sont équivalents (les\n",
    "prédictions ici sont aléatoires car les paramètres « model » ont été\n",
    "initialisés de manière aléatoire lors de la création de « model »)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "846e8337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9135],\n",
       "        [ 0.8532],\n",
       "        [ 0.7398],\n",
       "        [ 0.7149],\n",
       "        [ 0.5838],\n",
       "        [ 0.3852],\n",
       "        [ 0.2221],\n",
       "        [ 0.0981],\n",
       "        [-0.2742],\n",
       "        [-0.3274]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)\n",
    "model.forward(x)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260bc098",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant construire notre processus d'entraînement. Nous utiliserons `MSELoss()` comme fonction de perte\n",
    "et un optimiseur `SGD` avec un taux d'apprentissage de 0,1. Cependant,\n",
    "au lieu de passer explicitement une liste de paramètres comme `[w, b]` comme premier\n",
    "argument de l'optimiseur, nous utiliserons `model.parameters()` qui\n",
    "fournira automatiquement tous les paramètres de notre objet `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ce5e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3248f0e",
   "metadata": {},
   "source": [
    "Enfin, nous définissons et exécutons notre boucle d'entraînement pendant un certain nombre d'\n",
    "époches :\n",
    "\n",
    "-   nous commençons par réinitialiser notre gradient avec `optimizer.zero_grad()`\n",
    "-   nous calculons les valeurs prédites en appliquant notre objet `model` aux\n",
    "    données d'entrée (passage direct)\n",
    "- nous calculons la valeur de perte\n",
    "- nous calculons le gradient de perte pour chaque paramètre (rétropropagation)\n",
    "- enfin, nous ajustons nos paramètres en appelant `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4899e599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. loss:   207.3, weight: 89.50, bias: 202.55\n",
      " 2. loss:   199.6, weight: 89.61, bias: 202.80\n",
      " 3. loss:   192.3, weight: 89.72, bias: 203.04\n",
      " 4. loss:   185.3, weight: 89.82, bias: 203.28\n",
      " 5. loss:   178.5, weight: 89.93, bias: 203.51\n",
      " 6. loss:   172.0, weight: 90.03, bias: 203.74\n",
      " 7. loss:   165.8, weight: 90.13, bias: 203.97\n",
      " 8. loss:   159.8, weight: 90.23, bias: 204.19\n",
      " 9. loss:   154.0, weight: 90.32, bias: 204.40\n",
      "10. loss:   148.5, weight: 90.42, bias: 204.62\n",
      "11. loss:   143.2, weight: 90.51, bias: 204.82\n",
      "12. loss:   138.1, weight: 90.60, bias: 205.03\n",
      "13. loss:   133.2, weight: 90.69, bias: 205.23\n",
      "14. loss:   128.5, weight: 90.78, bias: 205.42\n",
      "15. loss:   124.0, weight: 90.86, bias: 205.61\n",
      "16. loss:   119.7, weight: 90.95, bias: 205.80\n",
      "17. loss:   115.5, weight: 91.03, bias: 205.98\n",
      "18. loss:   111.5, weight: 91.11, bias: 206.16\n",
      "19. loss:   107.7, weight: 91.19, bias: 206.34\n",
      "20. loss:   104.0, weight: 91.26, bias: 206.51\n",
      "21. loss:   100.4, weight: 91.34, bias: 206.68\n",
      "22. loss:    97.0, weight: 91.41, bias: 206.85\n",
      "23. loss:    93.8, weight: 91.48, bias: 207.01\n",
      "24. loss:    90.6, weight: 91.55, bias: 207.17\n",
      "25. loss:    87.6, weight: 91.62, bias: 207.33\n",
      "26. loss:    84.7, weight: 91.69, bias: 207.48\n",
      "27. loss:    81.9, weight: 91.76, bias: 207.63\n",
      "28. loss:    79.3, weight: 91.82, bias: 207.78\n",
      "29. loss:    76.7, weight: 91.89, bias: 207.92\n",
      "30. loss:    74.2, weight: 91.95, bias: 208.06\n",
      "31. loss:    71.9, weight: 92.01, bias: 208.20\n",
      "32. loss:    69.6, weight: 92.07, bias: 208.34\n",
      "33. loss:    67.4, weight: 92.13, bias: 208.47\n",
      "34. loss:    65.3, weight: 92.19, bias: 208.60\n",
      "35. loss:    63.3, weight: 92.25, bias: 208.73\n",
      "36. loss:    61.4, weight: 92.30, bias: 208.85\n",
      "37. loss:    59.5, weight: 92.36, bias: 208.97\n",
      "38. loss:    57.8, weight: 92.41, bias: 209.09\n",
      "39. loss:    56.0, weight: 92.46, bias: 209.21\n",
      "40. loss:    54.4, weight: 92.51, bias: 209.33\n",
      "41. loss:    52.8, weight: 92.56, bias: 209.44\n",
      "42. loss:    51.3, weight: 92.61, bias: 209.55\n",
      "43. loss:    49.9, weight: 92.66, bias: 209.66\n",
      "44. loss:    48.5, weight: 92.71, bias: 209.77\n",
      "45. loss:    47.1, weight: 92.76, bias: 209.87\n",
      "46. loss:    45.8, weight: 92.80, bias: 209.97\n",
      "47. loss:    44.6, weight: 92.85, bias: 210.07\n",
      "48. loss:    43.4, weight: 92.89, bias: 210.17\n",
      "49. loss:    42.2, weight: 92.93, bias: 210.27\n",
      "50. loss:    41.2, weight: 92.97, bias: 210.36\n",
      "51. loss:    40.1, weight: 93.02, bias: 210.45\n",
      "52. loss:    39.1, weight: 93.06, bias: 210.54\n",
      "53. loss:    38.1, weight: 93.10, bias: 210.63\n",
      "54. loss:    37.2, weight: 93.13, bias: 210.72\n",
      "55. loss:    36.3, weight: 93.17, bias: 210.80\n",
      "56. loss:    35.4, weight: 93.21, bias: 210.89\n",
      "57. loss:    34.6, weight: 93.25, bias: 210.97\n",
      "58. loss:    33.8, weight: 93.28, bias: 211.05\n",
      "59. loss:    33.0, weight: 93.32, bias: 211.13\n",
      "60. loss:    32.3, weight: 93.35, bias: 211.21\n",
      "61. loss:    31.6, weight: 93.38, bias: 211.28\n",
      "62. loss:    30.9, weight: 93.42, bias: 211.36\n",
      "63. loss:    30.3, weight: 93.45, bias: 211.43\n",
      "64. loss:    29.7, weight: 93.48, bias: 211.50\n",
      "65. loss:    29.1, weight: 93.51, bias: 211.57\n",
      "66. loss:    28.5, weight: 93.54, bias: 211.64\n",
      "67. loss:    27.9, weight: 93.57, bias: 211.70\n",
      "68. loss:    27.4, weight: 93.60, bias: 211.77\n",
      "69. loss:    26.9, weight: 93.63, bias: 211.83\n",
      "70. loss:    26.4, weight: 93.66, bias: 211.90\n",
      "71. loss:    25.9, weight: 93.68, bias: 211.96\n",
      "72. loss:    25.5, weight: 93.71, bias: 212.02\n",
      "73. loss:    25.0, weight: 93.74, bias: 212.08\n",
      "74. loss:    24.6, weight: 93.76, bias: 212.14\n",
      "75. loss:    24.2, weight: 93.79, bias: 212.19\n",
      "76. loss:    23.8, weight: 93.81, bias: 212.25\n",
      "77. loss:    23.5, weight: 93.84, bias: 212.30\n",
      "78. loss:    23.1, weight: 93.86, bias: 212.36\n",
      "79. loss:    22.8, weight: 93.89, bias: 212.41\n",
      "80. loss:    22.5, weight: 93.91, bias: 212.46\n",
      "81. loss:    22.1, weight: 93.93, bias: 212.51\n",
      "82. loss:    21.8, weight: 93.95, bias: 212.56\n",
      "83. loss:    21.6, weight: 93.97, bias: 212.61\n",
      "84. loss:    21.3, weight: 94.00, bias: 212.66\n",
      "85. loss:    21.0, weight: 94.02, bias: 212.70\n",
      "86. loss:    20.8, weight: 94.04, bias: 212.75\n",
      "87. loss:    20.5, weight: 94.06, bias: 212.79\n",
      "88. loss:    20.3, weight: 94.08, bias: 212.84\n",
      "89. loss:    20.0, weight: 94.09, bias: 212.88\n",
      "90. loss:    19.8, weight: 94.11, bias: 212.92\n",
      "91. loss:    19.6, weight: 94.13, bias: 212.96\n",
      "92. loss:    19.4, weight: 94.15, bias: 213.00\n",
      "93. loss:    19.2, weight: 94.17, bias: 213.04\n",
      "94. loss:    19.0, weight: 94.18, bias: 213.08\n",
      "95. loss:    18.9, weight: 94.20, bias: 213.12\n",
      "96. loss:    18.7, weight: 94.22, bias: 213.16\n",
      "97. loss:    18.5, weight: 94.23, bias: 213.19\n",
      "98. loss:    18.4, weight: 94.25, bias: 213.23\n",
      "99. loss:    18.2, weight: 94.27, bias: 213.26\n",
      "100. loss:    18.1, weight: 94.28, bias: 213.30\n",
      "101. loss:    17.9, weight: 94.30, bias: 213.33\n",
      "102. loss:    17.8, weight: 94.31, bias: 213.36\n",
      "103. loss:    17.7, weight: 94.32, bias: 213.40\n",
      "104. loss:    17.5, weight: 94.34, bias: 213.43\n",
      "105. loss:    17.4, weight: 94.35, bias: 213.46\n",
      "106. loss:    17.3, weight: 94.37, bias: 213.49\n",
      "107. loss:    17.2, weight: 94.38, bias: 213.52\n",
      "108. loss:    17.1, weight: 94.39, bias: 213.55\n",
      "109. loss:    17.0, weight: 94.41, bias: 213.58\n",
      "110. loss:    16.9, weight: 94.42, bias: 213.61\n",
      "111. loss:    16.8, weight: 94.43, bias: 213.63\n",
      "112. loss:    16.7, weight: 94.44, bias: 213.66\n",
      "113. loss:    16.6, weight: 94.45, bias: 213.69\n",
      "114. loss:    16.5, weight: 94.47, bias: 213.71\n",
      "115. loss:    16.5, weight: 94.48, bias: 213.74\n",
      "116. loss:    16.4, weight: 94.49, bias: 213.76\n",
      "117. loss:    16.3, weight: 94.50, bias: 213.79\n",
      "118. loss:    16.2, weight: 94.51, bias: 213.81\n",
      "119. loss:    16.2, weight: 94.52, bias: 213.83\n",
      "120. loss:    16.1, weight: 94.53, bias: 213.86\n",
      "121. loss:    16.1, weight: 94.54, bias: 213.88\n",
      "122. loss:    16.0, weight: 94.55, bias: 213.90\n",
      "123. loss:    15.9, weight: 94.56, bias: 213.92\n",
      "124. loss:    15.9, weight: 94.57, bias: 213.94\n",
      "125. loss:    15.8, weight: 94.58, bias: 213.96\n",
      "126. loss:    15.8, weight: 94.59, bias: 213.99\n",
      "127. loss:    15.7, weight: 94.60, bias: 214.01\n",
      "128. loss:    15.7, weight: 94.60, bias: 214.02\n",
      "129. loss:    15.6, weight: 94.61, bias: 214.04\n",
      "130. loss:    15.6, weight: 94.62, bias: 214.06\n",
      "131. loss:    15.5, weight: 94.63, bias: 214.08\n",
      "132. loss:    15.5, weight: 94.64, bias: 214.10\n",
      "133. loss:    15.5, weight: 94.65, bias: 214.12\n",
      "134. loss:    15.4, weight: 94.65, bias: 214.13\n",
      "135. loss:    15.4, weight: 94.66, bias: 214.15\n",
      "136. loss:    15.4, weight: 94.67, bias: 214.17\n",
      "137. loss:    15.3, weight: 94.67, bias: 214.18\n",
      "138. loss:    15.3, weight: 94.68, bias: 214.20\n",
      "139. loss:    15.3, weight: 94.69, bias: 214.21\n",
      "140. loss:    15.2, weight: 94.70, bias: 214.23\n",
      "141. loss:    15.2, weight: 94.70, bias: 214.25\n",
      "142. loss:    15.2, weight: 94.71, bias: 214.26\n",
      "143. loss:    15.2, weight: 94.72, bias: 214.27\n",
      "144. loss:    15.1, weight: 94.72, bias: 214.29\n",
      "145. loss:    15.1, weight: 94.73, bias: 214.30\n",
      "146. loss:    15.1, weight: 94.73, bias: 214.32\n",
      "147. loss:    15.1, weight: 94.74, bias: 214.33\n",
      "148. loss:    15.0, weight: 94.75, bias: 214.34\n",
      "149. loss:    15.0, weight: 94.75, bias: 214.35\n",
      "150. loss:    15.0, weight: 94.76, bias: 214.37\n",
      "151. loss:    15.0, weight: 94.76, bias: 214.38\n",
      "152. loss:    15.0, weight: 94.77, bias: 214.39\n",
      "153. loss:    15.0, weight: 94.77, bias: 214.40\n",
      "154. loss:    14.9, weight: 94.78, bias: 214.41\n",
      "155. loss:    14.9, weight: 94.78, bias: 214.43\n",
      "156. loss:    14.9, weight: 94.79, bias: 214.44\n",
      "157. loss:    14.9, weight: 94.79, bias: 214.45\n",
      "158. loss:    14.9, weight: 94.80, bias: 214.46\n",
      "159. loss:    14.9, weight: 94.80, bias: 214.47\n",
      "160. loss:    14.9, weight: 94.81, bias: 214.48\n",
      "161. loss:    14.8, weight: 94.81, bias: 214.49\n",
      "162. loss:    14.8, weight: 94.82, bias: 214.50\n",
      "163. loss:    14.8, weight: 94.82, bias: 214.51\n",
      "164. loss:    14.8, weight: 94.82, bias: 214.52\n",
      "165. loss:    14.8, weight: 94.83, bias: 214.53\n",
      "166. loss:    14.8, weight: 94.83, bias: 214.54\n",
      "167. loss:    14.8, weight: 94.84, bias: 214.55\n",
      "168. loss:    14.8, weight: 94.84, bias: 214.55\n",
      "169. loss:    14.8, weight: 94.84, bias: 214.56\n",
      "170. loss:    14.7, weight: 94.85, bias: 214.57\n",
      "171. loss:    14.7, weight: 94.85, bias: 214.58\n",
      "172. loss:    14.7, weight: 94.85, bias: 214.59\n",
      "173. loss:    14.7, weight: 94.86, bias: 214.60\n",
      "174. loss:    14.7, weight: 94.86, bias: 214.60\n",
      "175. loss:    14.7, weight: 94.86, bias: 214.61\n",
      "176. loss:    14.7, weight: 94.87, bias: 214.62\n",
      "177. loss:    14.7, weight: 94.87, bias: 214.62\n",
      "178. loss:    14.7, weight: 94.87, bias: 214.63\n",
      "179. loss:    14.7, weight: 94.88, bias: 214.64\n",
      "180. loss:    14.7, weight: 94.88, bias: 214.65\n",
      "181. loss:    14.7, weight: 94.88, bias: 214.65\n",
      "182. loss:    14.7, weight: 94.89, bias: 214.66\n",
      "183. loss:    14.7, weight: 94.89, bias: 214.67\n",
      "184. loss:    14.7, weight: 94.89, bias: 214.67\n",
      "185. loss:    14.7, weight: 94.89, bias: 214.68\n",
      "186. loss:    14.6, weight: 94.90, bias: 214.68\n",
      "187. loss:    14.6, weight: 94.90, bias: 214.69\n",
      "188. loss:    14.6, weight: 94.90, bias: 214.70\n",
      "189. loss:    14.6, weight: 94.91, bias: 214.70\n",
      "190. loss:    14.6, weight: 94.91, bias: 214.71\n",
      "191. loss:    14.6, weight: 94.91, bias: 214.71\n",
      "192. loss:    14.6, weight: 94.91, bias: 214.72\n",
      "193. loss:    14.6, weight: 94.91, bias: 214.72\n",
      "194. loss:    14.6, weight: 94.92, bias: 214.73\n",
      "195. loss:    14.6, weight: 94.92, bias: 214.73\n",
      "196. loss:    14.6, weight: 94.92, bias: 214.74\n",
      "197. loss:    14.6, weight: 94.92, bias: 214.74\n",
      "198. loss:    14.6, weight: 94.93, bias: 214.75\n",
      "199. loss:    14.6, weight: 94.93, bias: 214.75\n",
      "200. loss:    14.6, weight: 94.93, bias: 214.76\n",
      "201. loss:    14.6, weight: 94.93, bias: 214.76\n",
      "202. loss:    14.6, weight: 94.93, bias: 214.77\n",
      "203. loss:    14.6, weight: 94.94, bias: 214.77\n",
      "204. loss:    14.6, weight: 94.94, bias: 214.77\n",
      "205. loss:    14.6, weight: 94.94, bias: 214.78\n",
      "206. loss:    14.6, weight: 94.94, bias: 214.78\n",
      "207. loss:    14.6, weight: 94.94, bias: 214.79\n",
      "208. loss:    14.6, weight: 94.94, bias: 214.79\n",
      "209. loss:    14.6, weight: 94.95, bias: 214.79\n",
      "210. loss:    14.6, weight: 94.95, bias: 214.80\n",
      "211. loss:    14.6, weight: 94.95, bias: 214.80\n",
      "212. loss:    14.6, weight: 94.95, bias: 214.80\n",
      "213. loss:    14.6, weight: 94.95, bias: 214.81\n",
      "214. loss:    14.6, weight: 94.95, bias: 214.81\n",
      "215. loss:    14.6, weight: 94.96, bias: 214.82\n",
      "216. loss:    14.6, weight: 94.96, bias: 214.82\n",
      "217. loss:    14.6, weight: 94.96, bias: 214.82\n",
      "218. loss:    14.6, weight: 94.96, bias: 214.82\n",
      "219. loss:    14.6, weight: 94.96, bias: 214.83\n",
      "220. loss:    14.6, weight: 94.96, bias: 214.83\n",
      "221. loss:    14.6, weight: 94.96, bias: 214.83\n",
      "222. loss:    14.6, weight: 94.97, bias: 214.84\n",
      "223. loss:    14.6, weight: 94.97, bias: 214.84\n",
      "224. loss:    14.6, weight: 94.97, bias: 214.84\n",
      "225. loss:    14.6, weight: 94.97, bias: 214.85\n",
      "226. loss:    14.6, weight: 94.97, bias: 214.85\n",
      "227. loss:    14.6, weight: 94.97, bias: 214.85\n",
      "228. loss:    14.6, weight: 94.97, bias: 214.85\n",
      "229. loss:    14.6, weight: 94.97, bias: 214.86\n",
      "230. loss:    14.6, weight: 94.98, bias: 214.86\n",
      "231. loss:    14.6, weight: 94.98, bias: 214.86\n",
      "232. loss:    14.6, weight: 94.98, bias: 214.86\n",
      "233. loss:    14.6, weight: 94.98, bias: 214.87\n",
      "234. loss:    14.6, weight: 94.98, bias: 214.87\n",
      "235. loss:    14.6, weight: 94.98, bias: 214.87\n",
      "236. loss:    14.6, weight: 94.98, bias: 214.87\n",
      "237. loss:    14.6, weight: 94.98, bias: 214.87\n",
      "238. loss:    14.6, weight: 94.98, bias: 214.88\n",
      "239. loss:    14.6, weight: 94.98, bias: 214.88\n",
      "240. loss:    14.6, weight: 94.99, bias: 214.88\n",
      "241. loss:    14.6, weight: 94.99, bias: 214.88\n",
      "242. loss:    14.6, weight: 94.99, bias: 214.88\n",
      "243. loss:    14.6, weight: 94.99, bias: 214.89\n",
      "244. loss:    14.6, weight: 94.99, bias: 214.89\n",
      "245. loss:    14.6, weight: 94.99, bias: 214.89\n",
      "246. loss:    14.5, weight: 94.99, bias: 214.89\n",
      "247. loss:    14.5, weight: 94.99, bias: 214.89\n",
      "248. loss:    14.5, weight: 94.99, bias: 214.90\n",
      "249. loss:    14.5, weight: 94.99, bias: 214.90\n",
      "250. loss:    14.5, weight: 94.99, bias: 214.90\n",
      "251. loss:    14.5, weight: 94.99, bias: 214.90\n",
      "252. loss:    14.5, weight: 94.99, bias: 214.90\n",
      "253. loss:    14.5, weight: 95.00, bias: 214.90\n",
      "254. loss:    14.5, weight: 95.00, bias: 214.91\n",
      "255. loss:    14.5, weight: 95.00, bias: 214.91\n",
      "256. loss:    14.5, weight: 95.00, bias: 214.91\n",
      "257. loss:    14.5, weight: 95.00, bias: 214.91\n",
      "258. loss:    14.5, weight: 95.00, bias: 214.91\n",
      "259. loss:    14.5, weight: 95.00, bias: 214.91\n",
      "260. loss:    14.5, weight: 95.00, bias: 214.91\n",
      "261. loss:    14.5, weight: 95.00, bias: 214.91\n",
      "262. loss:    14.5, weight: 95.00, bias: 214.92\n",
      "263. loss:    14.5, weight: 95.00, bias: 214.92\n",
      "264. loss:    14.5, weight: 95.00, bias: 214.92\n",
      "265. loss:    14.5, weight: 95.00, bias: 214.92\n",
      "266. loss:    14.5, weight: 95.00, bias: 214.92\n",
      "267. loss:    14.5, weight: 95.00, bias: 214.92\n",
      "268. loss:    14.5, weight: 95.00, bias: 214.92\n",
      "269. loss:    14.5, weight: 95.00, bias: 214.92\n",
      "270. loss:    14.5, weight: 95.01, bias: 214.93\n",
      "271. loss:    14.5, weight: 95.01, bias: 214.93\n",
      "272. loss:    14.5, weight: 95.01, bias: 214.93\n",
      "273. loss:    14.5, weight: 95.01, bias: 214.93\n",
      "274. loss:    14.5, weight: 95.01, bias: 214.93\n",
      "275. loss:    14.5, weight: 95.01, bias: 214.93\n",
      "276. loss:    14.5, weight: 95.01, bias: 214.93\n",
      "277. loss:    14.5, weight: 95.01, bias: 214.93\n",
      "278. loss:    14.5, weight: 95.01, bias: 214.93\n",
      "279. loss:    14.5, weight: 95.01, bias: 214.93\n",
      "280. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "281. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "282. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "283. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "284. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "285. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "286. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "287. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "288. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "289. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "290. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "291. loss:    14.5, weight: 95.01, bias: 214.94\n",
      "292. loss:    14.5, weight: 95.01, bias: 214.95\n",
      "293. loss:    14.5, weight: 95.01, bias: 214.95\n",
      "294. loss:    14.5, weight: 95.01, bias: 214.95\n",
      "295. loss:    14.5, weight: 95.01, bias: 214.95\n",
      "296. loss:    14.5, weight: 95.01, bias: 214.95\n",
      "297. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "298. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "299. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "300. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "301. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "302. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "303. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "304. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "305. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "306. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "307. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "308. loss:    14.5, weight: 95.02, bias: 214.95\n",
      "309. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "310. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "311. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "312. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "313. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "314. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "315. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "316. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "317. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "318. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "319. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "320. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "321. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "322. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "323. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "324. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "325. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "326. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "327. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "328. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "329. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "330. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "331. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "332. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "333. loss:    14.5, weight: 95.02, bias: 214.96\n",
      "334. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "335. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "336. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "337. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "338. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "339. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "340. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "341. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "342. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "343. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "344. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "345. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "346. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "347. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "348. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "349. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "350. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "351. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "352. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "353. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "354. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "355. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "356. loss:    14.5, weight: 95.02, bias: 214.97\n",
      "357. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "358. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "359. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "360. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "361. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "362. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "363. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "364. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "365. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "366. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "367. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "368. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "369. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "370. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "371. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "372. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "373. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "374. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "375. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "376. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "377. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "378. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "379. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "380. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "381. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "382. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "383. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "384. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "385. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "386. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "387. loss:    14.5, weight: 95.03, bias: 214.97\n",
      "388. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "389. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "390. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "391. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "392. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "393. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "394. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "395. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "396. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "397. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "398. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "399. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "400. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "401. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "402. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "403. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "404. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "405. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "406. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "407. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "408. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "409. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "410. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "411. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "412. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "413. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "414. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "415. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "416. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "417. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "418. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "419. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "420. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "421. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "422. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "423. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "424. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "425. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "426. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "427. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "428. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "429. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "430. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "431. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "432. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "433. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "434. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "435. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "436. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "437. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "438. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "439. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "440. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "441. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "442. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "443. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "444. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "445. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "446. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "447. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "448. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "449. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "450. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "451. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "452. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "453. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "454. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "455. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "456. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "457. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "458. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "459. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "460. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "461. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "462. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "463. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "464. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "465. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "466. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "467. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "468. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "469. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "470. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "471. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "472. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "473. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "474. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "475. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "476. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "477. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "478. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "479. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "480. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "481. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "482. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "483. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "484. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "485. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "486. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "487. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "488. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "489. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "490. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "491. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "492. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "493. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "494. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "495. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "496. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "497. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "498. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "499. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "500. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "501. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "502. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "503. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "504. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "505. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "506. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "507. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "508. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "509. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "510. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "511. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "512. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "513. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "514. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "515. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "516. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "517. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "518. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "519. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "520. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "521. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "522. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "523. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "524. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "525. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "526. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "527. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "528. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "529. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "530. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "531. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "532. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "533. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "534. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "535. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "536. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "537. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "538. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "539. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "540. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "541. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "542. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "543. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "544. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "545. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "546. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "547. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "548. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "549. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "550. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "551. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "552. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "553. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "554. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "555. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "556. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "557. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "558. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "559. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "560. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "561. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "562. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "563. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "564. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "565. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "566. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "567. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "568. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "569. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "570. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "571. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "572. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "573. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "574. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "575. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "576. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "577. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "578. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "579. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "580. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "581. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "582. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "583. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "584. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "585. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "586. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "587. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "588. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "589. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "590. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "591. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "592. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "593. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "594. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "595. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "596. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "597. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "598. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "599. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "600. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "601. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "602. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "603. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "604. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "605. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "606. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "607. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "608. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "609. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "610. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "611. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "612. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "613. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "614. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "615. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "616. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "617. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "618. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "619. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "620. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "621. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "622. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "623. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "624. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "625. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "626. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "627. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "628. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "629. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "630. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "631. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "632. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "633. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "634. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "635. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "636. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "637. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "638. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "639. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "640. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "641. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "642. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "643. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "644. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "645. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "646. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "647. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "648. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "649. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "650. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "651. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "652. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "653. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "654. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "655. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "656. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "657. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "658. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "659. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "660. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "661. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "662. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "663. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "664. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "665. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "666. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "667. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "668. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "669. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "670. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "671. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "672. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "673. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "674. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "675. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "676. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "677. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "678. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "679. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "680. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "681. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "682. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "683. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "684. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "685. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "686. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "687. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "688. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "689. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "690. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "691. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "692. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "693. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "694. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "695. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "696. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "697. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "698. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "699. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "700. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "701. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "702. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "703. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "704. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "705. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "706. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "707. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "708. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "709. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "710. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "711. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "712. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "713. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "714. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "715. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "716. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "717. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "718. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "719. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "720. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "721. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "722. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "723. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "724. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "725. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "726. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "727. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "728. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "729. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "730. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "731. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "732. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "733. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "734. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "735. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "736. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "737. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "738. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "739. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "740. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "741. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "742. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "743. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "744. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "745. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "746. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "747. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "748. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "749. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "750. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "751. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "752. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "753. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "754. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "755. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "756. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "757. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "758. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "759. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "760. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "761. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "762. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "763. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "764. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "765. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "766. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "767. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "768. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "769. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "770. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "771. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "772. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "773. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "774. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "775. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "776. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "777. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "778. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "779. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "780. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "781. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "782. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "783. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "784. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "785. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "786. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "787. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "788. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "789. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "790. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "791. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "792. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "793. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "794. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "795. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "796. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "797. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "798. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "799. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "800. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "801. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "802. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "803. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "804. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "805. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "806. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "807. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "808. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "809. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "810. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "811. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "812. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "813. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "814. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "815. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "816. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "817. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "818. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "819. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "820. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "821. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "822. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "823. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "824. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "825. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "826. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "827. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "828. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "829. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "830. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "831. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "832. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "833. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "834. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "835. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "836. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "837. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "838. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "839. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "840. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "841. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "842. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "843. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "844. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "845. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "846. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "847. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "848. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "849. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "850. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "851. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "852. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "853. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "854. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "855. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "856. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "857. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "858. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "859. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "860. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "861. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "862. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "863. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "864. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "865. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "866. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "867. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "868. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "869. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "870. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "871. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "872. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "873. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "874. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "875. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "876. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "877. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "878. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "879. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "880. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "881. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "882. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "883. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "884. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "885. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "886. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "887. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "888. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "889. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "890. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "891. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "892. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "893. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "894. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "895. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "896. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "897. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "898. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "899. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "900. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "901. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "902. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "903. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "904. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "905. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "906. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "907. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "908. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "909. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "910. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "911. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "912. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "913. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "914. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "915. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "916. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "917. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "918. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "919. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "920. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "921. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "922. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "923. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "924. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "925. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "926. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "927. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "928. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "929. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "930. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "931. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "932. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "933. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "934. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "935. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "936. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "937. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "938. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "939. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "940. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "941. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "942. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "943. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "944. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "945. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "946. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "947. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "948. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "949. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "950. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "951. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "952. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "953. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "954. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "955. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "956. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "957. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "958. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "959. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "960. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "961. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "962. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "963. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "964. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "965. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "966. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "967. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "968. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "969. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "970. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "971. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "972. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "973. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "974. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "975. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "976. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "977. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "978. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "979. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "980. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "981. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "982. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "983. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "984. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "985. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "986. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "987. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "988. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "989. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "990. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "991. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "992. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "993. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "994. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "995. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "996. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "997. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "998. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "999. loss:    14.5, weight: 95.03, bias: 214.98\n",
      "1000. loss:    14.5, weight: 95.03, bias: 214.98\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Mettre le modèle en mode entraînement - important pour la normalisation par lots et les\n",
    "    # couches de dropout. Inutile dans cette situation mais ajouté pour les meilleures pratiques\n",
    "    model.train()\n",
    "    # Réinitialiser les gradients des paramètres à chaque itération\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: calculer les prédictions\n",
    "    y_pred = model(x)\n",
    "    # Calculer la loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    # Ajuster les paramètres\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\n",
    "        f\"{epoch + 1:2}. loss: {loss:7.1f}, weight: {model.linear.weight.item():5.2f},\"\n",
    "        f\" bias: {model.linear.bias.item():6.2f}\"\n",
    "    )\n",
    "\n",
    "#slope: 95.03, intercept: 214.98, mse: 14.5403"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a81589",
   "metadata": {},
   "source": [
    "On peut voir, avec beaucoup d'epoc, que le modèle atteint les vrai valeurs calculer précédemment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a6682",
   "metadata": {},
   "source": [
    "# Regression avec deux variables explicatives\n",
    "\n",
    "Si nous voulons effectuer une régression linéaire avec deux variables explicatives, nos\n",
    "données d'entrée `X` seront désormais un tableau à deux colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "300a0c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5, 50.1],\n",
       "       [ 0.2, 34.8],\n",
       "       [ 3.4, 51.3],\n",
       "       [ 4.1, 64.1],\n",
       "       [ 7.8, 47.8],\n",
       "       [13.4, 53.4],\n",
       "       [18. , 58. ],\n",
       "       [21.5, 71.5],\n",
       "       [32. , 32. ],\n",
       "       [33.5, 43.5]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input data\n",
    "temperature = [-1.5, 0.2, 3.4, 4.1, 7.8, 13.4, 18.0, 21.5, 32.0, 33.5]\n",
    "humidity = [50.1, 34.8, 51.3, 64.1, 47.8, 53.4, 58.0, 71.5, 32.0, 43.5]\n",
    "icecream = [100.5, 110.2, 133.5, 141.2, 172.8, 225.1, 251.0, 278.9, 366.7, 369.9]\n",
    "\n",
    "\n",
    "X = np.array([temperature, humidity]).transpose()\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1f03c6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2238, -0.0476],\n",
      "        [-1.0827, -1.3712],\n",
      "        [-0.8170,  0.0562],\n",
      "        [-0.7589,  1.1635],\n",
      "        [-0.4517, -0.2466],\n",
      "        [ 0.0133,  0.2379],\n",
      "        [ 0.3952,  0.6358],\n",
      "        [ 0.6858,  1.8037],\n",
      "        [ 1.5576, -1.6134],\n",
      "        [ 1.6822, -0.6185]])\n",
      "tensor([[100.5000],\n",
      "        [110.2000],\n",
      "        [133.5000],\n",
      "        [141.2000],\n",
      "        [172.8000],\n",
      "        [225.1000],\n",
      "        [251.0000],\n",
      "        [278.9000],\n",
      "        [366.7000],\n",
      "        [369.9000]])\n"
     ]
    }
   ],
   "source": [
    "X = preprocessing.scale(X)\n",
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(icecream).float().view(-1, 1) # .view permet de reshaper le tenseur en une colonne\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae8690",
   "metadata": {},
   "source": [
    "On peut maintenant normaliser nos données et les convertir en tensor comme d'habitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97620c5b",
   "metadata": {},
   "source": [
    "**Exercice 1**\n",
    "\n",
    "-   Créez une nouvelle classe de modèle `LinearNetwork2` qui aura la même\n",
    "    architecture que `LinearNetwork` mais qui acceptera des données d'entrée de\n",
    "    dimension 2.\n",
    "-   Créez un nouvel objet `model2` à partir de la classe `LinearNetwork2`\n",
    "-   Affichez une description sommaire de `model2`\n",
    "-   Exécutez une boucle d'apprentissage de `model2` pendant 20 époques avec une perte `MSELoss`\n",
    "    et un optimiseur `SGD` avec un taux d'apprentissage de 0,1\n",
    "\n",
    "*Astuce :* si vous souhaitez afficher les valeurs des poids et des biais à chaque\n",
    "époque, vous pouvez utiliser `model2.linear.weight.data` et\n",
    "`model2.linear.bias.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c87e9b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "LinearNetwork2                           --\n",
       "├─Linear: 1-1                            3\n",
       "=================================================================\n",
       "Total params: 3\n",
       "Trainable params: 3\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "class LinearNetwork2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(in_features=2, out_features=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "model2 = LinearNetwork2()\n",
    "summary(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd4c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. loss: 55166.4, weight: tensor([[18.4822, -2.7803]]), bias:  43.39\n",
      " 2. loss: 35264.2, weight: tensor([[33.7173, -4.3484]]), bias:  77.71\n",
      " 3. loss: 22560.5, weight: tensor([[45.8634, -5.1955]]), bias: 105.17\n",
      " 4. loss: 14445.7, weight: tensor([[55.5576, -5.5485]]), bias: 127.13\n",
      " 5. loss:  9258.4, weight: tensor([[63.3036, -5.5716]]), bias: 144.70\n",
      " 6. loss:  5940.0, weight: tensor([[69.4998, -5.3830]]), bias: 158.75\n",
      " 7. loss:  3815.6, weight: tensor([[74.4617, -5.0665]]), bias: 170.00\n",
      " 8. loss:  2454.6, weight: tensor([[78.4398, -4.6806]]), bias: 179.00\n",
      " 9. loss:  1582.1, weight: tensor([[81.6325, -4.2656]]), bias: 186.19\n",
      "10. loss:  1022.3, weight: tensor([[84.1978, -3.8481]]), bias: 191.95\n",
      "11. loss:   662.9, weight: tensor([[86.2612, -3.4456]]), bias: 196.56\n",
      "12. loss:   432.0, weight: tensor([[87.9227, -3.0684]]), bias: 200.24\n",
      "13. loss:   283.5, weight: tensor([[89.2619, -2.7222]]), bias: 203.19\n",
      "14. loss:   187.9, weight: tensor([[90.3426, -2.4095]]), bias: 205.55\n",
      "15. loss:   126.4, weight: tensor([[91.2155, -2.1304]]), bias: 207.43\n",
      "16. loss:    86.7, weight: tensor([[91.9213, -1.8838]]), bias: 208.94\n",
      "17. loss:    61.2, weight: tensor([[92.4925, -1.6676]]), bias: 210.15\n",
      "18. loss:    44.7, weight: tensor([[92.9553, -1.4794]]), bias: 211.12\n",
      "19. loss:    34.0, weight: tensor([[93.3305, -1.3165]]), bias: 211.89\n",
      "20. loss:    27.1, weight: tensor([[93.6350, -1.1761]]), bias: 212.51\n"
     ]
    }
   ],
   "source": [
    "model2.forward(X)\n",
    "\n",
    "loss_fn2 = nn.MSELoss() # fonction de perte distance entre y_pred et y \n",
    "lr2 = 0.1 \n",
    "optimizer2=torch.optim.SGD(model2.parameters(),lr=lr2)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    optimizer2.zero_grad()\n",
    "    y_pred=model2(X)\n",
    "    loss = loss_fn2(y_pred,y)\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    print(f\"{epoch + 1:2}. loss: {loss:7.1f}, weight: {model2.linear.weight.data},\"\n",
    "          f\" bias: {model2.linear.bias.item():6.2f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea965750",
   "metadata": {},
   "source": [
    "# Interprétation \n",
    "\n",
    "Pour la lecture des résultats, ici on peut prendre directement les poids et donner : \n",
    "\n",
    "- $ventes_glaces = 93 × (température_norm) - 1.16 × (humidité_norm) + 212.5$\n",
    "\n",
    "- On peut soit donner une interprétation direct disant que la température est le facteur déterminer pour la vente de glace ou alors si on veut des coefficients interprétables, il faut recalculer les coefficients pour les données originales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef50bc",
   "metadata": {},
   "source": [
    "# Généralisation à un nombre quelconque de variables explicatives\n",
    "\n",
    "**Exercice 2**\n",
    "\n",
    "Nous avons créé deux classes différentes ci-dessus : une pour un modèle de régression linéaire\n",
    "avec une seule variable explicative, et une pour deux variables explicatives.\n",
    " Nous allons maintenant essayer de créer une classe de modèle plus générique qui peut\n",
    "renvoyer des modèles acceptant un nombre quelconque de variables explicatives.\n",
    "\n",
    "- Créez une nouvelle classe `GeneralLinearNetwork` en partant de la\n",
    "  classe `LinearNetwork` vue ci-dessus\n",
    "- Modifiez la méthode `__init__()` afin qu'elle accepte un nouvel argument\n",
    "  appelé `n_variables`\n",
    "- Modifiez la création de `self.linear` afin qu'elle prenne en compte la\n",
    "  valeur passée en tant qu'argument `n_variables`\n",
    "\n",
    "Une fois la classe créée :\n",
    "\n",
    "- Instanciez un objet modèle appelé `model1` qui accepte des données d'entrée\n",
    "  avec une colonne et appliquez-le aux données d'entrée `x`.\n",
    "- Instanciez un objet modèle appelé `model2` qui accepte des données d'entrée\n",
    "  avec deux colonnes et appliquez-le aux données d'entrée `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "06677bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data,model,lr,optimizer,loss_fn,epochs):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()\n",
    "      y_pred=model(data)\n",
    "      loss = loss_fn(y_pred,y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      print(f\"{epoch + 1:2}. loss: {loss:7.1f}, weight: {model.linear.weight.data},\"\n",
    "          f\" bias: {model.linear.bias.item():6.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c0dc0727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. loss: 55526.9, weight: tensor([[19.4812]]), bias:  42.29\n",
      " 2. loss: 35542.4, weight: tensor([[34.5908]]), bias:  76.83\n",
      " 3. loss: 22752.4, weight: tensor([[46.6785]]), bias: 104.46\n",
      " 4. loss: 14566.8, weight: tensor([[56.3486]]), bias: 126.56\n",
      " 5. loss:  9328.0, weight: tensor([[64.0848]]), bias: 144.25\n",
      " 6. loss:  5975.1, weight: tensor([[70.2737]]), bias: 158.39\n",
      " 7. loss:  3829.3, weight: tensor([[75.2248]]), bias: 169.71\n",
      " 8. loss:  2456.0, weight: tensor([[79.1857]]), bias: 178.77\n",
      " 9. loss:  1577.1, weight: tensor([[82.3544]]), bias: 186.01\n",
      "10. loss:  1014.6, weight: tensor([[84.8894]]), bias: 191.80\n",
      "11. loss:   654.6, weight: tensor([[86.9173]]), bias: 196.44\n",
      "12. loss:   424.1, weight: tensor([[88.5397]]), bias: 200.15\n",
      "13. loss:   276.7, weight: tensor([[89.8376]]), bias: 203.11\n",
      "14. loss:   182.3, weight: tensor([[90.8759]]), bias: 205.49\n",
      "15. loss:   121.9, weight: tensor([[91.7066]]), bias: 207.39\n",
      "16. loss:    83.3, weight: tensor([[92.3711]]), bias: 208.90\n",
      "17. loss:    58.5, weight: tensor([[92.9027]]), bias: 210.12\n",
      "18. loss:    42.7, weight: tensor([[93.3280]]), bias: 211.09\n",
      "19. loss:    32.6, weight: tensor([[93.6683]]), bias: 211.87\n",
      "20. loss:    26.1, weight: tensor([[93.9405]]), bias: 212.49\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GeneralLinearNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_variables):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features=n_variables,out_features=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "        \n",
    "model1 = GeneralLinearNetwork(1)\n",
    "model1(x) # model1.forward(x) : identique\n",
    "loss_fn = nn.MSELoss() # fonction de perte distance entre y_pred et y \n",
    "lr = 0.1 \n",
    "\n",
    "epochs = 20\n",
    "optimizer1=torch.optim.SGD(model1.parameters(),lr=lr)\n",
    "\n",
    "train_model(x,model1,lr,optimizer1,loss_fn,epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3cdd362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. loss: 55134.3, weight: tensor([[18.6782, -2.7963]]), bias:  43.37\n",
      " 2. loss: 35244.1, weight: tensor([[33.8736, -4.3559]]), bias:  77.69\n",
      " 3. loss: 22547.9, weight: tensor([[45.9883, -5.1974]]), bias: 105.15\n",
      " 4. loss: 14437.7, weight: tensor([[55.6575, -5.5466]]), bias: 127.11\n",
      " 5. loss:  9253.3, weight: tensor([[63.3836, -5.5675]]), bias: 144.69\n",
      " 6. loss:  5936.7, weight: tensor([[69.5638, -5.3776]]), bias: 158.75\n",
      " 7. loss:  3813.5, weight: tensor([[74.5131, -5.0604]]), bias: 169.99\n",
      " 8. loss:  2453.3, weight: tensor([[78.4811, -4.6744]]), bias: 178.99\n",
      " 9. loss:  1581.2, weight: tensor([[81.6657, -4.2595]]), bias: 186.19\n",
      "10. loss:  1021.7, weight: tensor([[84.2245, -3.8424]]), bias: 191.95\n",
      "11. loss:   662.5, weight: tensor([[86.2827, -3.4403]]), bias: 196.55\n",
      "12. loss:   431.7, weight: tensor([[87.9400, -3.0636]]), bias: 200.24\n",
      "13. loss:   283.3, weight: tensor([[89.2760, -2.7179]]), bias: 203.19\n",
      "14. loss:   187.8, weight: tensor([[90.3540, -2.4057]]), bias: 205.55\n",
      "15. loss:   126.3, weight: tensor([[91.2247, -2.1270]]), bias: 207.43\n",
      "16. loss:    86.7, weight: tensor([[91.9287, -1.8808]]), bias: 208.94\n",
      "17. loss:    61.1, weight: tensor([[92.4985, -1.6651]]), bias: 210.15\n",
      "18. loss:    44.6, weight: tensor([[92.9602, -1.4772]]), bias: 211.12\n",
      "19. loss:    34.0, weight: tensor([[93.3345, -1.3146]]), bias: 211.89\n",
      "20. loss:    27.1, weight: tensor([[93.6383, -1.1745]]), bias: 212.51\n"
     ]
    }
   ],
   "source": [
    "model2 = GeneralLinearNetwork(2)\n",
    "model2(X) # model2.forward(X) : identique\n",
    "loss_fn2 = nn.MSELoss() # fonction de perte distance entre y_pred et y \n",
    "lr2 = 0.1 \n",
    "optimizer2=torch.optim.SGD(model2.parameters(),lr=lr2)\n",
    "\n",
    "epochs2 = 20\n",
    "\n",
    "train_model(X,model2,lr2,optimizer2,loss_fn2,epochs2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
